// üöÄ GOD-TIER MCP Server ‚Äì TypeScript Implementation
//
// ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÄ‡∏ï‡πá‡∏°‡∏™‡∏π‡∏ö:
// ‚úÖ MCP-compatible SSE endpoint (/sse) ‡∏û‡∏£‡πâ‡∏≠‡∏° streaming tokens
// ‚úÖ Anthropic-compatible /v1/messages proxy ‚Üí OpenAI GPT (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Claude Code)
// ‚úÖ Long-term memory (RAG): JSON vector store + OpenAI embeddings
// ‚úÖ Short-term session memory (ring buffer per client)
// ‚úÖ Tool calling: web_fetch (allowlist) + file_read (allowlist) + ‡∏ä‡∏∏‡∏î OpenAI tools
// ‚úÖ Bearer auth, rate limiting, CORS, request logging
// ‚úÖ SSE streaming ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö /v1/messages (real-time response)
// ‚úÖ Error handling ‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î + logging ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
// ‚úÖ Zero external DB - ‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á
//
// ‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô:
//   1) npm i express express-rate-limit cors dotenv openai node-fetch@2
//      npm i -D typescript ts-node @types/node @types/express @types/cors @types/node-fetch
//   2) ‡∏™‡∏£‡πâ‡∏≤‡∏á .env:
//      OPENAI_API_KEY=sk-...
//      MODEL=gpt-4o-mini
//      PORT=8787
//      BEARER=dev-secret
//      RAG_PATH=./rag_store.json
//      ALLOW_WEB_FETCH=https://api.github.com,https://example.com
//      ALLOW_FILE_READ=./docs,./notes
//   3) npx ts-node server.ts
//   4) ChatGPT Dev ‚Üí MCP Server URL: http://localhost:8787/sse
//   5) Claude Code (VS Code settings.json):
//      {
//        "anthropic.api_url": "http://localhost:8787",
//        "anthropic.api_key": "test-key"
//      }
//
// NOTE: Single-file ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏á‡πà‡∏≤‡∏¢ - Production ‡∏Ñ‡∏ß‡∏£‡πÅ‡∏¢‡∏Å‡πÄ‡∏õ‡πá‡∏ô modules

import cors from 'cors';
import dotenv from 'dotenv';
import express from 'express';
import rateLimit from 'express-rate-limit';
import fs from 'fs';
import fetch from 'node-fetch';
import { Configuration, OpenAIApi } from 'openai';
import path from 'path';

dotenv.config();

const PORT = Number(process.env.PORT || 8787);
const OPENAI_API_KEY = process.env.OPENAI_API_KEY || '';
const MODEL = process.env.MODEL || 'gpt-4o-mini';
const EMBEDDING_MODEL = process.env.EMBEDDING_MODEL || 'text-embedding-3-large';
const BEARER = process.env.BEARER; // optional
const RAG_PATH = process.env.RAG_PATH || './rag_store.json';
const ALLOW_WEB_FETCH = (process.env.ALLOW_WEB_FETCH || '').split(',').filter(Boolean);
const ALLOW_FILE_READ = (process.env.ALLOW_FILE_READ || '').split(',').filter(Boolean).map(p=>path.resolve(p));

const app = express();
app.use(cors());
app.use(express.json({ limit: '10mb' })); // ‡πÄ‡∏û‡∏¥‡πà‡∏° limit ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RAG documents

const limiter = rateLimit({ windowMs: 60_000, max: 200, message: '‡∏ñ‡∏π‡∏Å rate limit ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏£‡∏±‡∏ö ‡∏£‡∏≠‡πÅ‡∏õ‡πä‡∏ö‡∏ô‡∏∂‡∏á' });
app.use(limiter);

// Logger middleware
app.use((req, res, next) => {
  const start = Date.now();
  res.on('finish', () => {
    const duration = Date.now() - start;
    console.log(`[${new Date().toISOString()}] ${req.method} ${req.path} - ${res.statusCode} (${duration}ms)`);
  });
  next();
});

// --- Simple bearer auth middleware (optional) ---
function requireBearer(req: express.Request, res: express.Response, next: express.NextFunction) {
  if (!BEARER) return next();
  const auth = req.headers['authorization'] || '';
  if (auth === `Bearer ${BEARER}`) return next();
  return res.status(401).json({ error: 'Unauthorized' });
}

// --- OpenAI client (lazy init) ---
let openai: OpenAIApi | null = null;

function getOpenAI(): OpenAIApi {
  if (!openai) {
    if (!OPENAI_API_KEY || OPENAI_API_KEY === 'sk-...') {
      throw new Error('‚ö†Ô∏è  OPENAI_API_KEY ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á! ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà API key ‡πÉ‡∏ô .env');
    }
    openai = new OpenAIApi(new Configuration({ apiKey: OPENAI_API_KEY } as any));
  }
  return openai;
}

// --- Utility: cosine similarity ---
function cosine(a: number[], b: number[]) {
  let dot = 0, na = 0, nb = 0;
  for (let i = 0; i < a.length; i++) { dot += a[i]*b[i]; na += a[i]*a[i]; nb += b[i]*b[i]; }
  return dot / (Math.sqrt(na) * Math.sqrt(nb) + 1e-8);
}

// --- RAG Store (JSON on disk) ---
interface RagDoc { id: string; text: string; meta?: Record<string, any>; embedding?: number[]; }
interface RagStore { docs: RagDoc[] }

function loadRag(): RagStore {
  if (!fs.existsSync(RAG_PATH)) return { docs: [] };
  return JSON.parse(fs.readFileSync(RAG_PATH, 'utf-8')) as RagStore;
}
function saveRag(store: RagStore) { fs.writeFileSync(RAG_PATH, JSON.stringify(store, null, 2)); }
let RAG = loadRag();

async function embed(texts: string[]): Promise<number[][]> {
  const client = getOpenAI();
  const resp: any = await client.createEmbedding({ model: EMBEDDING_MODEL as any, input: texts as any });
  return resp.data.data.map((d: any) => d.embedding);
}

async function upsertDoc(text: string, meta?: Record<string, any>) {
  const [vec] = await embed([text]);
  const doc: RagDoc = { id: 'doc_' + Date.now() + '_' + Math.random().toString(36).slice(2), text, meta, embedding: vec };
  RAG.docs.push(doc);
  saveRag(RAG);
  return doc;
}

function searchRag(queryVec: number[], k = 5) {
  const scored = RAG.docs.filter(d=>d.embedding).map(d => ({ d, score: cosine(queryVec, d.embedding!)}));
  return scored.sort((a,b)=>b.score-a.score).slice(0,k);
}

// --- Short-term session memory ---
interface Msg { role: 'user'|'assistant'|'system'; content: string }
const SESSIONS: Record<string, Msg[]> = {};
function sessionGet(id: string) { return SESSIONS[id] ||= []; }
function sessionPush(id: string, m: Msg) { sessionGet(id).push(m); if (SESSIONS[id].length > 50) SESSIONS[id].shift(); }

// --- Tools (safe) ---
async function tool_web_fetch(url: string) {
  if (!ALLOW_WEB_FETCH.some(prefix => url.startsWith(prefix))) {
    throw new Error(`Blocked by allowlist. Allowed prefixes: ${ALLOW_WEB_FETCH.join(', ')}`);
  }
  const r = await fetch(url);
  const text = await r.text();
  return text.slice(0, 8000); // cap
}

function tool_file_read(fp: string) {
  const abs = path.resolve(fp);
  const allowed = ALLOW_FILE_READ.some(dir => abs.startsWith(dir));
  if (!allowed) throw new Error(`Blocked by allowlist. Allowed dirs: ${ALLOW_FILE_READ.join(', ')}`);
  return fs.readFileSync(abs, 'utf-8').slice(0, 8000);
}

// --- Prompt builder with RAG ---
async function buildMessages(sessionId: string, userText: string): Promise<Msg[]> {
  // 1) embed query and retrieve context
  const [qVec] = await embed([userText]);
  const top = searchRag(qVec, 5);
  const context = top.map(t => `# Doc (score=${t.score.toFixed(3)})\n${t.d.text}`).join('\n\n');

  // 2) compose system prompt
  const system: Msg = {
    role: 'system',
    content:
`You are a Godtier MCP server assistant. You have tools (web_fetch, file_read).\n\nGuidelines:\n- Use provided CONTEXT for factual answers.\n- If uncertain, say so.\n- Keep outputs concise but accurate.\n\nCONTEXT:\n${context}`
  };

  // 3) take last N messages for short-term memory
  const history = sessionGet(sessionId).slice(-12);
  return [system, ...history, { role: 'user', content: userText }];
}

// --- Core chat completion (OpenAI) with optional streaming ---
async function chat(messages: Msg[], stream = false): Promise<any> {
  const client = getOpenAI();
  const resp: any = await client.createChatCompletion({
    model: MODEL as any,
    messages: messages.map(m => ({ role: m.role, content: m.content }) ) as any,
    temperature: 0.7,
    stream,
  });

  if (stream) {
    return resp; // return stream object
  }
  return resp.data.choices[0].message?.content || '';
}// --- Tool definitions (OpenAI format) ---
const TOOLS = [
  {
    type: 'function',
    function: {
      name: 'web_fetch',
      description: '‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å URL ‡∏ó‡∏µ‡πà‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï (allowlist)',
      parameters: {
        type: 'object',
        properties: {
          url: { type: 'string', description: 'URL ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•' }
        },
        required: ['url']
      }
    }
  },
  {
    type: 'function',
    function: {
      name: 'file_read',
      description: '‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å‡πÑ‡∏î‡πÄ‡∏£‡∏Å‡∏ó‡∏≠‡∏£‡∏µ‡∏ó‡∏µ‡πà‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï (allowlist)',
      parameters: {
        type: 'object',
        properties: {
          path: { type: 'string', description: 'path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô' }
        },
        required: ['path']
      }
    }
  }
];

// --- Execute tool calls ---
async function executeTool(name: string, args: any) {
  try {
    if (name === 'web_fetch') {
      return await tool_web_fetch(args.url);
    } else if (name === 'file_read') {
      return tool_file_read(args.path);
    }
    throw new Error(`Unknown tool: ${name}`);
  } catch (e: any) {
    return `Error: ${e?.message || String(e)}`;
  }
}

// --- Streaming via SSE helper ---
function sseInit(res: express.Response) {
  res.setHeader('Content-Type', 'text/event-stream');
  res.setHeader('Cache-Control', 'no-cache');
  res.setHeader('Connection', 'keep-alive');
}
function sseSend(res: express.Response, event: string, data: any) {
  res.write(`event: ${event}\n`);
  res.write(`data: ${JSON.stringify(data)}\n\n`);
}

// --- MCP: SSE endpoint ---
app.get('/sse', requireBearer, async (req, res) => {
  sseInit(res);
  const sessionId = (req.query.session as string) || 'default';
  sseSend(res, 'ready', { message: 'MCP server ready', sessionId });

  // lightweight ping
  const iv = setInterval(() => sseSend(res, 'ping', { t: Date.now() }), 25_000);
  req.on('close', () => clearInterval(iv));
});

// --- Query endpoint used by connectors (POST /query) ---
app.post('/query', requireBearer, async (req, res) => {
  try {
    const { text, sessionId = 'default' } = req.body || {};
    if (!text) return res.status(400).json({ error: 'Missing text' });

    const msgs = await buildMessages(sessionId, text);
    const answer = await chat(msgs);

    // persist short-term memory
    sessionPush(sessionId, { role: 'user', content: text });
    sessionPush(sessionId, { role: 'assistant', content: answer });

    res.json({ sessionId, answer });
  } catch (e:any) {
    res.status(500).json({ error: e?.message || String(e) });
  }
});

// --- Anthropic-compatible proxy: /v1/messages (with SSE streaming support!) ---
// ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Claude Code + clients ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ Anthropic Messages API ‚Üí ‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ GPT
app.post('/v1/messages', requireBearer, async (req, res) => {
  try {
    const {
      model = MODEL,
      messages = [],
      max_tokens = 2048,
      temperature = 0.7,
      stream = false,
      tools = []
    } = req.body || {};

    // Convert Anthropic-style blocks -> OpenAI format
    const toOpenAI: Msg[] = (messages as any[]).map((m:any) => {
      const text = (m.content || []).filter((b:any)=>b.type==='text').map((b:any)=>b.text).join('\n');
      return { role: m.role, content: text } as Msg;
    });

    // ‡∏ñ‡πâ‡∏≤ Claude ‡∏™‡πà‡∏á tools ‡∏°‡∏≤ ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô OpenAI format
    const openaiTools = tools.length > 0 ? tools : (TOOLS as any);

    // SSE Streaming mode
    if (stream) {
      res.setHeader('Content-Type', 'text/event-stream');
      res.setHeader('Cache-Control', 'no-cache');
      res.setHeader('Connection', 'keep-alive');

      const client = getOpenAI();
      const resp: any = await client.createChatCompletion({
        model: model as any,
        max_tokens,
        temperature,
        messages: toOpenAI as any,
        stream: true,
      } as any, { responseType: 'stream' });

      let fullText = '';
      const msgId = 'msg_' + Date.now();

      resp.data.on('data', (chunk: Buffer) => {
        const lines = chunk.toString().split('\n').filter((line: string) => line.trim() !== '');
        for (const line of lines) {
          const message = line.replace(/^data: /, '');
          if (message === '[DONE]') {
            // Send final message
            const finalEvent = {
              type: 'message_stop',
              message: {
                id: msgId,
                type: 'message',
                role: 'assistant',
                content: [{ type: 'text', text: fullText }]
              }
            };
            res.write(`event: message_stop\ndata: ${JSON.stringify(finalEvent)}\n\n`);
            res.end();
            return;
          }
          try {
            const parsed = JSON.parse(message);
            const delta = parsed.choices?.[0]?.delta?.content;
            if (delta) {
              fullText += delta;
              // Send content delta in Anthropic format
              const event = {
                type: 'content_block_delta',
                index: 0,
                delta: { type: 'text_delta', text: delta }
              };
              res.write(`event: content_block_delta\ndata: ${JSON.stringify(event)}\n\n`);
            }
          } catch (e) {
            // ignore parsing errors
          }
        }
      });

      resp.data.on('error', (err: Error) => {
        res.write(`event: error\ndata: ${JSON.stringify({ error: err.message })}\n\n`);
        res.end();
      });

      return;
    }

    // Non-streaming mode (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
    const client = getOpenAI();
    const resp: any = await client.createChatCompletion({
      model: model as any,
      max_tokens,
      temperature,
      messages: toOpenAI as any,
      stream: false,
    } as any);

    const choice = resp.data.choices?.[0];
    const text = choice?.message?.content ?? '';
    const toolCalls = choice?.message?.tool_calls;

    // Handle tool calls if present
    let finalContent = text;
    if (toolCalls && toolCalls.length > 0) {
      const results = [];
      for (const tc of toolCalls) {
        const result = await executeTool(tc.function.name, JSON.parse(tc.function.arguments));
        results.push({ tool: tc.function.name, result });
      }
      finalContent = `${text}\n\n[Tool Results]\n${JSON.stringify(results, null, 2)}`;
    }

    const out = {
      id: 'msg_' + Date.now(),
      type: 'message',
      role: 'assistant',
      model,
      content: [{ type: 'text', text: finalContent }],
      stop_reason: toolCalls ? 'tool_use' : 'end_turn',
      usage: {
        input_tokens: resp.data.usage?.prompt_tokens ?? 0,
        output_tokens: resp.data.usage?.completion_tokens ?? 0,
      }
    };
    res.json(out);
  } catch (e:any) {
    console.error('‚ùå /v1/messages error:', e?.message || String(e));
    res.status(500).json({
      type: 'error',
      error: {
        type: 'api_error',
        message: e?.message || String(e)
      }
    });
  }
});

// --- Tools endpoints (exposed so MCP client can call) ---
app.post('/tool/web_fetch', requireBearer, async (req, res) => {
  try { const { url } = req.body || {}; const data = await tool_web_fetch(url); res.json({ data }); }
  catch (e:any) { res.status(400).json({ error: e?.message || String(e) }); }
});
app.post('/tool/file_read', requireBearer, (req, res) => {
  try { const { path: p } = req.body || {}; const data = tool_file_read(p); res.json({ data }); }
  catch (e:any) { res.status(400).json({ error: e?.message || String(e) }); }
});

// --- RAG admin ---
app.post('/admin/ingest', requireBearer, async (req, res) => {
  try {
    const { text, meta } = req.body || {};
    if (!text) return res.status(400).json({ error: 'text required' });
    const doc = await upsertDoc(text, meta);
    res.json({ ok: true, doc });
  } catch (e:any) {
    res.status(500).json({ error: e?.message || String(e) });
  }
});

app.get('/health', (req,res) => res.json({ ok: true }));

app.listen(PORT, () => {
  console.log('\n' + '='.repeat(70));
  console.log('üöÄ GOD-TIER MCP SERVER ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏£‡∏±‡∏ö!');
  console.log('='.repeat(70));
  console.log(`\nüìç Base URL:        http://localhost:${PORT}`);
  console.log(`\nüîå Endpoints:`);
  console.log(`   ‚îú‚îÄ SSE (MCP):           GET  /sse?session=xxx`);
  console.log(`   ‚îú‚îÄ Query API:           POST /query`);
  console.log(`   ‚îú‚îÄ Anthropic Proxy:     POST /v1/messages (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö streaming!)`);
  console.log(`   ‚îú‚îÄ RAG Ingest:          POST /admin/ingest`);
  console.log(`   ‚îú‚îÄ Tool: Web Fetch:     POST /tool/web_fetch`);
  console.log(`   ‚îú‚îÄ Tool: File Read:     POST /tool/file_read`);
  console.log(`   ‚îî‚îÄ Health Check:        GET  /health`);
  console.log(`\nüõ†Ô∏è  Tools Available:`);
  console.log(`   ‚îú‚îÄ web_fetch (allowlist: ${ALLOW_WEB_FETCH.length} domains)`);
  console.log(`   ‚îî‚îÄ file_read (allowlist: ${ALLOW_FILE_READ.length} directories)`);
  console.log(`\nüß† Memory:`);
  console.log(`   ‚îú‚îÄ Long-term (RAG):  ${RAG.docs.length} documents loaded`);
  console.log(`   ‚îî‚îÄ Short-term:       Session-based (max 50 messages)`);
  console.log(`\nüîê Security:`);
  console.log(`   ‚îú‚îÄ Bearer Auth:      ${BEARER ? '‚úÖ Enabled' : '‚ùå Disabled (‡πÇ‡∏´‡∏°‡∏î‡πÄ‡∏ó‡∏™)'}`);
  console.log(`   ‚îú‚îÄ Rate Limit:       200 req/min`);
  console.log(`   ‚îî‚îÄ CORS:             ‚úÖ Enabled`);
  console.log(`\nüí° Quick Start:`);
  console.log(`   Claude Code (settings.json):`);
  console.log(`   {`);
  console.log(`     "anthropic.api_url": "http://localhost:${PORT}",`);
  console.log(`     "anthropic.api_key": "test-key"`);
  console.log(`   }`);
  console.log(`\n   ChatGPT Dev MCP: http://localhost:${PORT}/sse`);
  console.log('\n' + '='.repeat(70) + '\n');
});
